<!doctype html>
<html>
  <head>
    <link rel="stylesheet" href="https://s3.amazonaws.com/codecademy-content/projects/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet' type='text/css'>
    <link href='style.css' rel='stylesheet' type='text/css'>

  </head>
  
  <body>
    <div class="header">
      <h1><strong>Karen Shay West's Home Page</strong></h1>
      <div id="photo"> <a href="KarenShayWestAgeAlmost52Dec2016.jpg"><img alt=""
            src="KarenShayWestAgeAlmost52Dec2016.jpg" style="border: 0px solid; width: 85px;
            height: 85px;"></a> </div>

      <ul></ul>
      <p id="name">Karen Shay West: Tel: (508) 844-9776</p>
      <a href="mailto:KarenWest15@gmail.com"><p id="email">KarenWest15@gmail.com</p></a>
      
      <p id="notice"><strong><br>UNDER CONSTRUCTION WEB SITE</strong></p>
	        
      <ul></ul>

    </div>

    <div class="nav">
      <div class="container">
	<ul>
          <li><a href="index.html">LINK TO HOME MAIN PAGE</a></li>
	  <li><a href="https://drive.google.com/open?id=0B0ao1LZvdelIeWZZVFBKRzBfV0k">LINK
	  TO RESUME (CV)</a></li>
	  <li><a href="http://www.linkedin.com/in/karenshaywest">LINK TO LINKEDIN
	  SITE</a></li>
	  <li><a href="https://github.com/KarenWest">LINK TO GIT HUB SITE</a></li>
	</ul>
      </div>
    </div>

    <div class="experience">

		  <br><h4>Python Project: Markov Decision Processes: policies, rewards and
			values, value iteration and Reinforcement Learning: 
			TD/Q Learning, Exploration and Approximation
			Reinforcement Learning Project: </h4><br>

		  <ul>
		    <li>We implemented Value Iteration and Q-learning, and
		      tested our agents first on GridWorld and later to a 
		      simulated robot controller (Crawler) Pacman.</li><br>
		    <li>Reinforcement learning: an agent that takes actions 
		      on an environment, and the output state and reward from 
		      the action are passed back to the agent that took the
		      action.</li><br>
		    <li>It is a Markov decision process,
		      non-determininsitic.</li><br>
		    <li>You have a set of states with a set of associated
		      actions, and a transition function, a probability of one 
		      of the successor outcomes, a model of the dynamics,
		      along  with the reward of the associated
		      transition.</li><br>
		    <li>Markov means that given the present state, the future
		      and past are independent.</li><br>
		    <li>For decision processes, Markov means action outcomes
		      depend only on the current state, just like search,
		      where the successor function could only depend on the 
		      current state and not its history.</li><br>
		    <li>Deterministic single agent search problems: wanted an 
		      optimal plan, or sequence of actions, start to
		      goal.</li><br>
		    <li>For Markov Decision processes, we want an optimal
		      policy for each state.</li><br>
		    <li>The idea was to create a Pacman that could learn from
		      his environment.</li><br>
		    <li>Once Pacman was done training, he should win very 
		      reliably in test games, since now he is exploiting his 
		      learned policy.</li><br>
		    <li>This worked for small but not medium grids.</li><br>
		    <li>To correct this, we had to implement a q learning
		      agent that learns weights for features of states, where 
		      many states might share the same features.</li><br>
		    <li>Approximate q learning assumes the existence of a 
		      feature function over state and action pairs.</li><br>
		    <li>Approximate-q-agent uses the an extractor to assign a 
		      single feature to every (game state,action)
		      pair.</li><br>
		    <li>Even much larger layouts should be no problem for 
		      approximate-q-learning agent.</li><br>
		    <li>We then had a learning Pacman agent for small, medium
		      or large grids.</li><br>

		  </ul>
		</div>
   <div class="footer">	
				 <p>Web Site By Karen Shay West</p>
      <p>Tel: (508) 844-9776</p>
      <a href="mailto:KarenWest15@gmail.com">KarenWest15@gmail.com</a>
    </div>
	</body>
</html>
